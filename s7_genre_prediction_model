# S7 – Genre Modelling (mel64, PANNs, PANNs+CLIP) with scaling
# We train a very simple Logistic Regression Model on the embeddings to determine viability. 


# Setup and Loading
from pathlib import Path
import numpy as np
import pandas as pd
import math
from collections import OrderedDict
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    classification_report,
    confusion_matrix
)
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import GroupShuffleSplit

# Configuration
root = Path.cwd().parent  # adjust if needed

# Main manifests
MAIN_MANIFEST = root / "downloads" / "batch_0001" / "2_s1_qc_manifest.csv" # Your main manifest file
SEG_MANIFEST  = root / "processed" / "segments_manifest_with_panns.csv" # Your segmented and embedded segment

print("Root       :", root)
print("Main       :", MAIN_MANIFEST)
print("Segments   :", SEG_MANIFEST)

# %%
df_parent = pd.read_csv(MAIN_MANIFEST)
df_seg    = pd.read_csv(SEG_MANIFEST)

print("Parent rows :", len(df_parent))
print("Segments rows:", len(df_seg))
print("\nSegments columns:")
print(df_seg.columns.tolist())

# --- Merge genre labels from parent manifest ---
# df_parent must contain "video_id" and "genre_guess"
if "video_id" not in df_parent.columns or "genre_guess" not in df_parent.columns:
    raise ValueError("Parent manifest must have 'video_id' and 'genre_guess' columns.")

df = df_seg.merge(
    df_parent[["video_id", "genre_guess"]],
    left_on="parent_video_id",
    right_on="video_id",
    how="left",
)

print("\nMerged shape:", df.shape)
print("Merged columns:", df.columns.tolist())

# --- Filtering step-by-step ---
print("\n---- Filtering ----")
start_rows = len(df)
print("Start rows:", start_rows)

# 1) seg_keep & seg_ok
mask_keep = df["seg_keep"].fillna(False) & df["seg_ok"].fillna(False)
df = df[mask_keep].copy()
print("After seg_keep & seg_ok:", len(df))

# 2) drop missing genre labels
missing_genre = df["genre_guess"].isna().sum()
print("Missing genre labels:", missing_genre)
df = df[~df["genre_guess"].isna()].copy()
print("After dropping missing genres:", len(df))

# 3) require embeddings & no errors
def no_error(col):
    return df[col].fillna("").eq("")

required_cols = [
    "audio_emb_path",    # mel64 embeddings
    "audio_panns_path",  # PANNs embeddings
    "video_emb_path",    # CLIP frame embeddings
]

for c in required_cols:
    if c not in df.columns:
        raise ValueError(f"Expected column '{c}' in segments manifest.")

mask_all_paths = (
    df["audio_emb_path"].notna() &
    df["audio_panns_path"].notna() &
    df["video_emb_path"].notna()
)

mask_no_err = (
    no_error("audio_emb_error") &
    no_error("audio_panns_error") &
    no_error("video_emb_error")
)

df = df[mask_all_paths & mask_no_err].copy()
print("After requiring PANNs+CLIP paths and no errors:", len(df))

# 4) minimum samples per genre
genre_counts = df["genre_guess"].value_counts()
print("\nGenre counts BEFORE min filter:\n", genre_counts)

MIN_PER_CLASS = 30
valid_genres = genre_counts[genre_counts >= MIN_PER_CLASS].index.tolist()
df = df[df["genre_guess"].isin(valid_genres)].copy()

genre_counts_after = df["genre_guess"].value_counts()
print("\nGenre counts AFTER min filter:\n", genre_counts_after)
print("Final modelling df shape:", df.shape)

# Label encoding
label_names = sorted(df["genre_guess"].unique())
label_to_idx = {g: i for i, g in enumerate(label_names)}
idx_to_label = {i: g for g, i in label_to_idx.items()}

df["genre_label_idx"] = df["genre_guess"].map(label_to_idx).astype(int)
n_classes = len(label_names)

print("Label classes:", label_names)
print("n_classes:", n_classes)

# --- Grouped split by parent_video_id ---
groups = df["parent_video_id"].values

# First split: train vs temp (val+test)
gss1 = GroupShuffleSplit(n_splits=1, train_size=0.6, random_state=42)
train_idx, temp_idx = next(gss1.split(df, groups=groups))

df_train = df.iloc[train_idx].reset_index(drop=True)
df_temp  = df.iloc[temp_idx].reset_index(drop=True)

# Second split: val vs test from temp
gss2 = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=43)
temp_groups = df_temp["parent_video_id"].values
val_idx, test_idx = next(gss2.split(df_temp, groups=temp_groups))

df_val  = df_temp.iloc[val_idx].reset_index(drop=True)
df_test = df_temp.iloc[test_idx].reset_index(drop=True)

print("\n=== Split summary ===")
print(f"Rows      | train={len(df_train)}, val={len(df_val)}, test={len(df_test)}")
print(f"Parents   | train={df_train['parent_video_id'].nunique()}, "
      f"val={df_val['parent_video_id'].nunique()}, "
      f"test={df_test['parent_video_id'].nunique()}")

print("\nTrain genre distribution:")
print(df_train["genre_guess"].value_counts())
print("\nVal genre distribution:")
print(df_val["genre_guess"].value_counts())
print("\nTest genre distribution:")
print(df_test["genre_guess"].value_counts())


def load_emb(path_str: str) -> np.ndarray:
    """Load a .npy embedding and flatten to 1D float32."""
    arr = np.load(path_str)
    arr = np.asarray(arr).reshape(-1)
    return arr.astype(np.float32)

def build_feature_matrix(df_in: pd.DataFrame,
                         mode: str = "mel64") -> np.ndarray:
    """
    mode: 'mel64', 'panns', 'panns_clip'
    """
    feats = []
    for _, row in df_in.iterrows():
        if mode == "mel64":
            v = load_emb(row["audio_emb_path"])       # (64,)
        elif mode == "panns":
            v = load_emb(row["audio_panns_path"])     # (2048,)
        elif mode == "panns_clip":
            v1 = load_emb(row["audio_panns_path"])    # (2048,)
            v2 = load_emb(row["video_emb_path"])      # (512,)
            v = np.concatenate([v1, v2], axis=0)      # (2560,)
        else:
            raise ValueError(f"Unknown mode: {mode}")
        feats.append(v)
    X = np.stack(feats, axis=0)
    return X

y_train = df_train["genre_label_idx"].values
y_val   = df_val["genre_label_idx"].values
y_test  = df_test["genre_label_idx"].values

# Build all three sets
print("\nBuilding feature matrices...")

X_train_mel = build_feature_matrix(df_train, mode="mel64")
X_val_mel   = build_feature_matrix(df_val,   mode="mel64")
X_test_mel  = build_feature_matrix(df_test,  mode="mel64")

X_train_p   = build_feature_matrix(df_train, mode="panns")
X_val_p     = build_feature_matrix(df_val,   mode="panns")
X_test_p    = build_feature_matrix(df_test,  mode="panns")

X_train_pc  = build_feature_matrix(df_train, mode="panns_clip")
X_val_pc    = build_feature_matrix(df_val,   mode="panns_clip")
X_test_pc   = build_feature_matrix(df_test,  mode="panns_clip")

print("mel64 shapes:", X_train_mel.shape, X_val_mel.shape, X_test_mel.shape)
print("PANNs shapes:", X_train_p.shape, X_val_p.shape, X_test_p.shape)
print("PANNs+CLIP shapes:", X_train_pc.shape, X_val_pc.shape, X_test_pc.shape)


def evaluate_model(name, clf, X_tr, y_tr, X_val, y_val, X_te, y_te):
    """
    Fit classifier, evaluate on VAL & TEST.
    Returns dict with metrics, probabilities, predictions.
    """
    print(f"\n=== Training {name} model ===")
    clf.fit(X_tr, y_tr)

    proba_val = clf.predict_proba(X_val)
    proba_te  = clf.predict_proba(X_te)

    y_val_pred = proba_val.argmax(axis=1)
    y_te_pred  = proba_te.argmax(axis=1)

    def split_report(split_name, y_true, y_pred):
        acc = accuracy_score(y_true, y_pred)
        macro_f1 = f1_score(y_true, y_pred, average="macro")
        print(f"\n=== {name} ({split_name}) ===")
        print(f"Accuracy   : {acc:.3f}")
        print(f"Macro F1   : {macro_f1:.3f}")
        print("\nClassification report:")
        print(
            classification_report(
                y_true,
                y_pred,
                target_names=label_names,
                zero_division=0
            )
        )
        return acc, macro_f1

    acc_v, f1_v = split_report("VAL",  y_val, y_val_pred)
    acc_t, f1_t = split_report("TEST", y_te,  y_te_pred)

    return {
        "clf": clf,
        "proba_val": proba_val,
        "proba_test": proba_te,
        "y_val_pred": y_val_pred,
        "y_test_pred": y_te_pred,
        "metrics": {
            "VAL":  {"acc": acc_v, "macro_f1": f1_v},
            "TEST": {"acc": acc_t, "macro_f1": f1_t},
        },
    }


def plot_conf_mat(y_true, y_pred, labels, title, normalize=True):
    cm = confusion_matrix(y_true, y_pred)
    if normalize:
        cm = cm.astype(float) / cm.sum(axis=1, keepdims=True)
        cm = np.nan_to_num(cm)

    plt.figure(figsize=(8, 8))
    im = plt.imshow(cm, interpolation="nearest", cmap="viridis")
    plt.title(title)
    plt.colorbar(im, fraction=0.046, pad=0.04)
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=45, ha="right")
    plt.yticks(tick_marks, labels)

    thresh = cm.max() / 2.0
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            txt = f"{cm[i, j]:.2f}" if normalize else str(int(cm[i, j]))
            plt.text(
                j, i, txt,
                ha="center", va="center",
                color="white" if cm[i, j] > thresh else "black",
                fontsize=7
            )
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
    plt.tight_layout()
    plt.show()


def per_class_f1_barplot(y_true, y_pred, labels, title):
    rep = classification_report(
        y_true,
        y_pred,
        target_names=labels,
        zero_division=0,
        output_dict=True
    )
    f1s = [rep[label]["f1-score"] for label in labels]

    plt.figure(figsize=(10, 4))
    plt.bar(labels, f1s)
    plt.ylim(0.0, 1.0)
    plt.ylabel("F1 Score")
    plt.title(title)
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()


def reliability_diagram(y_true, y_pred, proba_max, n_bins=10, title="Reliability diagram"):
    """
    Simple reliability diagram + ECE using top-1 confidence.
    """
    bins = np.linspace(0.0, 1.0, n_bins + 1)
    bin_ids = np.digitize(proba_max, bins) - 1

    bin_conf = []
    bin_acc  = []
    bin_counts = []

    for b in range(n_bins):
        mask = bin_ids == b
        if not np.any(mask):
            continue
        conf_b = proba_max[mask].mean()
        acc_b  = (y_true[mask] == y_pred[mask]).mean()
        bin_conf.append(conf_b)
        bin_acc.append(acc_b)
        bin_counts.append(mask.sum())

    bin_conf   = np.array(bin_conf)
    bin_acc    = np.array(bin_acc)
    bin_counts = np.array(bin_counts)
    N = len(y_true)

    ece = np.sum(bin_counts * np.abs(bin_acc - bin_conf)) / N

    plt.figure(figsize=(5, 5))
    plt.plot([0, 1], [0, 1], "--", label="Perfect calibration")
    plt.plot(bin_conf, bin_acc, marker="o", label="Model")
    plt.xlabel("Predicted confidence")
    plt.ylabel("Empirical accuracy")
    plt.title(f"{title}\nECE={ece:.3f}")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

    return ece


def compute_genre_at_k(embeddings, labels, ks=(1, 5, 10)):
    """
    Retrieval metric: for each item, check if at least one of the
    top-k nearest neighbours (cosine similarity, excluding self)
    has the same genre label.
    """
    sim = cosine_similarity(embeddings)
    np.fill_diagonal(sim, -1.0)

    results = {}
    for k in ks:
        hits = []
        for i in range(len(embeddings)):
            idx = np.argpartition(-sim[i], k)[:k]
            hit = np.any(labels[idx] == labels[i])
            hits.append(1.0 if hit else 0.0)
        hits = np.array(hits)
        results[k] = (hits.mean(), hits.std())
    return results


results = OrderedDict()
rows_for_table = []

# Helper to fit scaler & model for a given feature mode
def run_model_for_mode(mode_name, mode_key):
    """
    mode_key in {"mel64", "panns", "panns_clip"}
    """
    print(f"\n\n###############################")
    print(f"### {mode_name} ({mode_key})")
    print(f"###############################")

    # 1) build raw features
    X_train_raw = build_feature_matrix(df_train, mode=mode_key)
    X_val_raw   = build_feature_matrix(df_val,   mode=mode_key)
    X_test_raw  = build_feature_matrix(df_test,  mode=mode_key)

    print(f"{mode_key} shapes:",
          X_train_raw.shape, X_val_raw.shape, X_test_raw.shape)

    # 2) Standard scaling (fit on train, apply to val & test)
    scaler = StandardScaler()
    scaler.fit(X_train_raw)

    X_train = scaler.transform(X_train_raw)
    X_val   = scaler.transform(X_val_raw)
    X_test  = scaler.transform(X_test_raw)

    # 3) Logistic regression
    clf = LogisticRegression(
        max_iter=5000,
        multi_class="multinomial",
        solver="lbfgs",
        n_jobs=-1,
    )

    res = evaluate_model(
        name=mode_name,
        clf=clf,
        X_tr=X_train, y_tr=y_train,
        X_val=X_val,  y_val=y_val,
        X_te=X_test,  y_te=y_test,
    )

    # store everything, including scaler & raw features (for retrieval)
    res["scaler"] = scaler
    res["X_train_raw"] = X_train_raw
    res["X_val_raw"]   = X_val_raw
    res["X_test_raw"]  = X_test_raw
    res["X_train"]     = X_train
    res["X_val"]       = X_val
    res["X_test"]      = X_test

    results[mode_key] = res

    # add to summary table
    rows_for_table.append({
        "model": mode_name,
        "split": "VAL",
        "acc":   res["metrics"]["VAL"]["acc"],
        "macro_f1": res["metrics"]["VAL"]["macro_f1"],
    })
    rows_for_table.append({
        "model": mode_name,
        "split": "TEST",
        "acc":   res["metrics"]["TEST"]["acc"],
        "macro_f1": res["metrics"]["TEST"]["macro_f1"],
    })


# Run for all three feature sets
run_model_for_mode("mel64",      "mel64")
run_model_for_mode("PANNs",      "panns")
run_model_for_mode("PANNs+CLIP", "panns_clip")

summary_df = pd.DataFrame(rows_for_table)
print("\n=== Summary table ===")
print(summary_df)


# Focus on the strongest model (PANNs+CLIP)
res_mc = results["panns_clip"]

proba_test = res_mc["proba_test"]
y_test_pred = res_mc["y_test_pred"]

conf_max = proba_test.max(axis=1)  # top-1 confidence

# Reliability diagram + ECE
ece = reliability_diagram(
    y_true=y_test,
    y_pred=y_test_pred,
    proba_max=conf_max,
    n_bins=10,
    title="PANNs+CLIP (TEST) reliability"
)

print(f"PANNs+CLIP TEST ECE: {ece:.3f}")

# Optional: show distribution of confidence scores
plt.figure(figsize=(6, 4))
plt.hist(conf_max, bins=20, range=(0, 1))
plt.xlabel("Top-1 confidence")
plt.ylabel("Count")
plt.title("PANNs+CLIP TEST confidence distribution")
plt.tight_layout()
plt.show()

plot_conf_mat(
    y_true=y_test,
    y_pred=results["panns_clip"]["y_test_pred"],
    labels=label_names,
    title="PANNs+CLIP Confusion Matrix (TEST, normalized)",
    normalize=True,
)

per_class_f1_barplot(
    y_true=y_test,
    y_pred=results["panns_clip"]["y_test_pred"],
    labels=label_names,
    title="PANNs+CLIP per-class F1 (TEST)",
)

# PANNs retrieval
emb_panns_test = results["panns"]["X_test"]
ret_panns = compute_genre_at_k(emb_panns_test, y_test, ks=(1, 5, 10))

print("PANNs Audio→Audio retrieval (TEST):")
for k, (m, s) in ret_panns.items():
    print(f"  genre@{k}: mean={m:.3f}, std={s:.3f}")

# PANNs+CLIP retrieval
emb_mc_test = results["panns_clip"]["X_test"]
ret_mc = compute_genre_at_k(emb_mc_test, y_test, ks=(1, 5, 10))

print("\nPANNs+CLIP Audio+Video retrieval (TEST):")
for k, (m, s) in ret_mc.items():
    print(f"  genre@{k}: mean={m:.3f}, std={s:.3f}")

y_test_mel   = results["mel64"]["y_test_pred"]
y_test_panns = results["panns"]["y_test_pred"]
y_test_mc    = results["panns_clip"]["y_test_pred"]

agree_all = (
    (y_test_mel == y_test_panns) &
    (y_test_panns == y_test_mc)
)
agree_panns_mc = (y_test_panns == y_test_mc)

print(f"\nAgreement (all three models) on TEST: {agree_all.mean():.3f}")
print(f"Agreement (PANNs vs PANNs+CLIP) on TEST: {agree_panns_mc.mean():.3f}")

# Where PANNs+CLIP correct but mel64 wrong
correct_mc_wrong_mel = (
    (y_test_mc == y_test) &
    (y_test_mel != y_test)
).mean()
print(f"PANNs+CLIP correct while mel64 wrong: {correct_mc_wrong_mel:.3f}")

# Where PANNs+CLIP correct but PANNs wrong
correct_mc_wrong_panns = (
    (y_test_mc == y_test) &
    (y_test_panns != y_test)
).mean()
print(f"PANNs+CLIP correct while PANNs wrong: {correct_mc_wrong_panns:.3f}")


# %%
res_mc = results["panns_clip"]
proba_test = res_mc["proba_test"]        # (N_test, n_classes)
y_test_pred = res_mc["y_test_pred"]      # (N_test,)

# Top-1 confidence and predicted label (index + name)
conf_max = proba_test.max(axis=1)
pred_idx = y_test_pred
pred_genre = [idx_to_label[i] for i in pred_idx]

seg_pred_df = pd.DataFrame({
    "segment_id":       df_test["segment_id"].values,
    "parent_video_id":  df_test["parent_video_id"].values,
    "true_genre":       df_test["genre_guess"].values,
    "true_label_idx":   df_test["genre_label_idx"].values,
    "pred_label_idx":   pred_idx,
    "pred_genre":       pred_genre,
    "conf_top1":        conf_max,
})

# Optionally add top-3 label indices & probs for richer analysis
top_k = 3
topk_idx = np.argsort(-proba_test, axis=1)[:, :top_k]
topk_prob = np.take_along_axis(proba_test, topk_idx, axis=1)

for k in range(top_k):
    seg_pred_df[f"top{k+1}_label_idx"] = topk_idx[:, k]
    seg_pred_df[f"top{k+1}_label_name"] = [idx_to_label[i] for i in topk_idx[:, k]]
    seg_pred_df[f"top{k+1}_prob"] = topk_prob[:, k]

out_seg_pred_csv = root / "processed" / "s7_panns_clip_segment_predictions.csv"
seg_pred_df.to_csv(out_seg_pred_csv, index=False)
print("Wrote segment-level PANNs+CLIP predictions →", out_seg_pred_csv)
seg_pred_df.head()
